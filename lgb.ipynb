{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import gc\n",
    "import math\n",
    "from ark_nlp.dataset.base._sentence_classification_dataset import SentenceClassificationDataset\n",
    "from ark_nlp.factory.loss_function.focal_loss import FocalLoss\n",
    "from transformers import BertConfig, BertModel\n",
    "from ark_nlp.processor.tokenizer.transfomer import SentenceTokenizer\n",
    "from model.nezha.configuration_nezha import NeZhaConfig\n",
    "from model.nezha.modeling_nezha import NeZhaModel, NeZhaForSequenceClassification\n",
    "from tokenizer import BertSpanTokenizer\n",
    "from utils import WarmupLinearSchedule, seed_everything, get_default_bert_optimizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from task import Task\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "from model.model import BertForSequenceClassification, BertEnsambleForSequenceClassification, BertBiLSTMForSequenceClassification\n",
    "from data_process import text_enchance\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "from torch import nn\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtract(nn.Module):\n",
    "    def __init__(self, config, bert):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_size=config.hidden_size,\n",
    "                              hidden_size=config.hidden_size // 2,\n",
    "                              bidirectional=True,\n",
    "                              dropout=0.1,\n",
    "                              batch_first=True)\n",
    "\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        **kargs\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        output, (h_n, c_n) = self.bilstm(sequence_output)\n",
    "\n",
    "        pooled_output = output[:, 0]\n",
    "\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 1 ==========\n",
      "抽取train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1971/1971 [02:25<00:00, 13.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽取val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 493/493 [00:36<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽取test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:22<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 2 ==========\n",
      "抽取train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1971/1971 [02:24<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽取val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 493/493 [00:35<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽取test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:22<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 3 ==========\n",
      "抽取train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1971/1971 [02:24<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽取val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 493/493 [00:36<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽取test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:22<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 4 ==========\n",
      "抽取train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1971/1971 [02:24<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽取val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 493/493 [00:36<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽取test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:22<00:00, 13.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 5 ==========\n",
      "抽取train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1971/1971 [02:24<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽取val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 493/493 [00:36<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽取test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:22<00:00, 13.64it/s]\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_path = '../data/a_dataset/query_data.csv'\n",
    "        self.goods_data_path = '../data/a_dataset/goods_data.csv'\n",
    "        self.model_name_or_path = '../pretrain_model/uer_large/'\n",
    "        self.model_type = 'uer_denoise_bilstm'\n",
    "        self.checkpoint = './checkpoint/'\n",
    "        self.test_file = '../data/a_dataset/test_a.csv'\n",
    "        self.max_seq_len = 64\n",
    "        self.batch_size = 16\n",
    "        self.num_workers = 0\n",
    "        self.fold = 5\n",
    "        self.seed = 42\n",
    "        self.predict_model = ''\n",
    "        self.device = 'cuda:0'\n",
    "\n",
    "\n",
    "def build_model_and_tokenizer(args, num_labels):\n",
    "    tokenizer = BertSpanTokenizer(vocab=args.model_name_or_path,\n",
    "                                  max_seq_len=args.max_seq_len)\n",
    "    config = BertConfig.from_pretrained(args.model_name_or_path,\n",
    "                                        num_labels=num_labels)\n",
    "    bert = BertModel(config=config)\n",
    "    dl_module = FeatureExtract(config, bert)\n",
    "    return tokenizer, dl_module\n",
    "\n",
    "\n",
    "def extract_cv(args):\n",
    "    data_df = pd.read_csv(args.data_path)\n",
    "    goods_df = pd.read_csv(args.goods_data_path)\n",
    "    data_df = pd.concat([data_df, goods_df])\n",
    "    data_df['label'] = data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "    data_df['text'] = data_df['text'].apply(lambda x: text_enchance(x))\n",
    "    data_df = data_df.drop(data_df[(data_df['text'] == '')].index)\n",
    "\n",
    "    test_data_df = pd.read_csv(args.test_file)\n",
    "    test_data_df['label'] = 1\n",
    "    test_data_df['label'] = test_data_df['label'].apply(lambda x: str(x))\n",
    "    test_data_df['text'] = test_data_df['text'].apply(\n",
    "        lambda x: text_enchance(x))\n",
    "    test_data_df.loc[(test_data_df['text'] == ''), 'text'] = '比赛占位字符'\n",
    "\n",
    "    test_dataset = SentenceClassificationDataset(\n",
    "        test_data_df, categories=sorted(data_df['label'].unique()))\n",
    "    tokenizer, _ = build_model_and_tokenizer(\n",
    "        args, len(test_dataset.cat2id))\n",
    "\n",
    "    test_dataset.convert_to_ids(tokenizer)\n",
    "    test_generator = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=args.num_workers)\n",
    "\n",
    "    def extract(generator):\n",
    "        feature_vector = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs in tqdm(generator):\n",
    "                inputs['input_ids'] = inputs['input_ids'].to(\n",
    "                    torch.device(args.device))\n",
    "                inputs['attention_mask'] = inputs['attention_mask'].to(\n",
    "                    torch.device(args.device))\n",
    "                inputs['token_type_ids'] = inputs['token_type_ids'].to(\n",
    "                    torch.device(args.device))\n",
    "                inputs['label_ids'] = inputs['label_ids'].to(\n",
    "                    torch.device(args.device))\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                feature_vector.append(outputs.cpu().numpy())\n",
    "        feature_vector = np.vstack(feature_vector)\n",
    "        return feature_vector\n",
    "\n",
    "    kfold = StratifiedKFold(\n",
    "        n_splits=args.fold, shuffle=True, random_state=args.seed)\n",
    "    args.checkpoint = os.path.join(args.checkpoint, args.model_type)\n",
    "    model_type = args.model_type\n",
    "\n",
    "    train_data = []\n",
    "    dev_data = []\n",
    "    test_data = []\n",
    "\n",
    "    for fold, (train_idx, dev_idx) in enumerate(kfold.split(data_df, data_df['label'])):\n",
    "        print(f'========== {fold + 1} ==========')\n",
    "        args.model_type = f'{model_type}-{fold + 1}'\n",
    "        args.predict_model = os.path.join(\n",
    "            args.checkpoint, args.model_type, 'best_model.pth')\n",
    "\n",
    "        train_data_df, dev_data_df = data_df.iloc[train_idx], data_df.iloc[dev_idx]\n",
    "        train_dataset = SentenceClassificationDataset(\n",
    "            train_data_df, categories=sorted(train_data_df['label'].unique()))\n",
    "        dev_dataset = SentenceClassificationDataset(\n",
    "            dev_data_df, categories=train_dataset.categories)\n",
    "\n",
    "        tokenizer, model = build_model_and_tokenizer(\n",
    "            args, len(train_dataset.cat2id))\n",
    "\n",
    "        train_dataset.convert_to_ids(tokenizer)\n",
    "        dev_dataset.convert_to_ids(tokenizer)\n",
    "\n",
    "        train_generator = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            pin_memory=True,\n",
    "            num_workers=args.num_workers)\n",
    "\n",
    "        dev_generator = DataLoader(\n",
    "            dev_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            pin_memory=True,\n",
    "            num_workers=args.num_workers)\n",
    "\n",
    "        model.load_state_dict(torch.load(args.predict_model), strict=False)\n",
    "        model.to(torch.device(args.device))\n",
    "        print('抽取train')\n",
    "        train_data.append([extract(train_generator), fold])\n",
    "        print('抽取val')\n",
    "        dev_data.append([extract(dev_generator), fold])\n",
    "        print('抽取test')\n",
    "        test_data.append([extract(test_generator), fold])\n",
    "\n",
    "        del model, tokenizer\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "train_data, dev_data, test_data = extract_cv(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train_data, columns=['vector', 'fold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.58633494, 0.02467776, -0.7460927, -0.6725...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.29947793, 0.28890938, 0.30287006, -0.6648...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.66576487, 0.32492572, 0.021158926, -0.672...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.0023959056, 1.0785404e-05, -0.608786, -0.0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.30451664, 0.14475796, -0.75497985, 0.01594...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              vector  fold\n",
       "0  [[-0.58633494, 0.02467776, -0.7460927, -0.6725...     0\n",
       "1  [[-0.29947793, 0.28890938, 0.30287006, -0.6648...     1\n",
       "2  [[-0.66576487, 0.32492572, 0.021158926, -0.672...     2\n",
       "3  [[0.0023959056, 1.0785404e-05, -0.608786, -0.0...     3\n",
       "4  [[0.30451664, 0.14475796, -0.75497985, 0.01594...     4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f31cdafcee8aa313d249a62a118f9dafafe7a3c4d7d99fbe10d547330ed6086"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
